{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cr√©ation du Dataset en scrappant les infos depuis une liste d'url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marque</th>\n",
       "      <th>Parfum</th>\n",
       "      <th>Lien du Parfum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Avon</td>\n",
       "      <td>Life</td>\n",
       "      <td>https://www.fragrantica.com/perfume/Avon/Princ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Marque Parfum                                     Lien du Parfum\n",
       "count      98     98                                                 98\n",
       "unique      1     97                                                 98\n",
       "top      Avon   Life  https://www.fragrantica.com/perfume/Avon/Princ...\n",
       "freq       98      2                                                  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scraping_info_perfume as sip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "df = pd.read_csv('list_url_by_popular_designer.csv')\n",
    "Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "urls_deja_scrapees = Fragrantica_dataset['url'].tolist()\n",
    "\n",
    "# Choix de la lettre √† scraper\n",
    "lettre = ('A')\n",
    "df_a_scraper = df[df['Marque'].str.startswith(lettre)]\n",
    "# Exclure les lignes d√©j√† pr√©sentes dans Fragrantica_dataset\n",
    "df_a_scraper = df_a_scraper[~df_a_scraper['Lien du Parfum'].isin(urls_deja_scrapees)] \n",
    "df_a_scraper.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pour voir le temps d'attente optimal √† scraper 50 parfums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x000002523881C040>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tapri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\selenium\\webdriver\\common\\service.py\", line 200, in __del__\n",
      "    self.stop()\n",
      "  File \"C:\\Users\\tapri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\selenium\\webdriver\\common\\service.py\", line 157, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"C:\\Users\\tapri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\selenium\\webdriver\\common\\service.py\", line 137, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 214, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 517, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 534, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 494, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 1375, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\", line 1346, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1285, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1331, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1280, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 1040, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 980, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\", line 946, in connect\n",
      "    self.sock = self._create_connection(\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\socket.py\", line 832, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'ex√©cution pour batch 1 : 566.3 secondes.\n",
      "Soit 11.6 secondes par URL.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 49\n",
    "\n",
    "# Charger le dataset existant\n",
    "Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "url_deja_scrape = set(Fragrantica_dataset['url'].tolist())\n",
    "\n",
    "# Filtrer les URLs d√©j√† scrap√©es\n",
    "df_a_scraper = df_a_scraper[~df_a_scraper['Lien du Parfum'].isin(url_deja_scrape)]\n",
    "\n",
    "# Initialiser les DataFrames pour les donn√©es scrap√©es\n",
    "scraped_data_1 = pd.DataFrame()\n",
    "scraped_data_2 = pd.DataFrame()\n",
    "\n",
    "# Fonction pour scraper un batch et mesurer le temps d'ex√©cution\n",
    "def scrape_batch(batch, batch_num):\n",
    "    liste_url = batch[\"Lien du Parfum\"].tolist()\n",
    "    start_time = time.time()\n",
    "    scraped_data = sip.scraping_multi_perfume_info(liste_url)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    scraped_data.to_csv(f\"scraped_data_{batch_num}.csv\", index=False)\n",
    "    return scraped_data, execution_time\n",
    "\n",
    "# Scraper le premier batch\n",
    "batch1 = df_a_scraper.iloc[0 : batch_size]\n",
    "scraped_data_1, execution_time_1 = scrape_batch(batch1, 1)\n",
    "print(f\"Temps d'ex√©cution pour batch 1 : {execution_time_1:.1f} secondes.\")\n",
    "print(f\"Soit {execution_time_1 / batch_size:.1f} secondes par URL.\")\n",
    "\n",
    "# Attendre avant de scraper le deuxi√®me batch\n",
    "time.sleep(1200)\n",
    "\n",
    "# Scraper le deuxi√®me batch\n",
    "batch2 = df_a_scraper.iloc[batch_size : 2 * batch_size]\n",
    "scraped_data_2, execution_time_2 = scrape_batch(batch2, 2)\n",
    "print(f\"Temps d'ex√©cution pour batch 2 : {execution_time_2:.1f} secondes.\")\n",
    "print(f\"Soit {execution_time_2 / batch_size:.1f} secondes par URL.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans Fragrantica_dataset : 1675\n",
      "Nombre de lignes dans Fragrantica_dataset : 1724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nscraped_data2 = pd.read_csv(f\"scraped_data_2.csv\")\\ncombined_data = pd.concat([Fragrantica_dataset, scraped_data2])\\ncombined_data.drop_duplicates(subset=[\\'url\\'], inplace=True)\\ncombined_data.to_csv(\"Fragrantica_dataset.csv\", index=False)\\n\\n\\nFragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\\nprint(f\"Nombre de lignes dans Fragrantica_dataset : {Fragrantica_dataset.shape[0]}\")\\nprint(f\"‚úÖ Les donn√©es ont √©t√© ajout√©es √† Fragrantica_dataset.csv sans doublons.\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "print(f\"Nombre de lignes dans Fragrantica_dataset : {Fragrantica_dataset.shape[0]}\")\n",
    "scraped_data1 = pd.read_csv(f\"scraped_data_1.csv\")\n",
    "#afficher le nomre de ligne de chaque dataframe\n",
    "# Concat√©ner les deux DataFrames\n",
    "combined_data = pd.concat([Fragrantica_dataset, scraped_data1])\n",
    "combined_data.drop_duplicates(subset=['url'], inplace=True)\n",
    "combined_data.to_csv(\"Fragrantica_dataset.csv\", index=False)\n",
    "\n",
    "Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "print(f\"Nombre de lignes dans Fragrantica_dataset : {Fragrantica_dataset.shape[0]}\")\n",
    "\n",
    "'''\n",
    "scraped_data2 = pd.read_csv(f\"scraped_data_2.csv\")\n",
    "combined_data = pd.concat([Fragrantica_dataset, scraped_data2])\n",
    "combined_data.drop_duplicates(subset=['url'], inplace=True)\n",
    "combined_data.to_csv(\"Fragrantica_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "print(f\"Nombre de lignes dans Fragrantica_dataset : {Fragrantica_dataset.shape[0]}\")\n",
    "print(f\"‚úÖ Les donn√©es ont √©t√© ajout√©es √† Fragrantica_dataset.csv sans doublons.\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le script qui scrape les infos par batch de 49 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ TAILLE DES BATCHS = 49\n",
      "üìÇ Nous avons d√©ja 17543 URLs dans le dataset Fragrantica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üì° Traitement du batch 1/2...\n",
      "üîç 0 URLs sur 49 ont d√©j√† √©t√© scrap√©es et retir√©es du batch 1.\n",
      "‚è±Ô∏è Temps d'ex√©cution du scraping sur le batch1 : 619.8 secondes.\n",
      "‚è±Ô∏è Soit un total de 12.6 secondes par URL.\n",
      "‚úÖ Batch 1 enregistr√© dans scrap_batch_1.csv.\n",
      "On attend 30 minutes\n",
      "\n",
      "üì° Traitement du batch 2/2...\n",
      "üîç 0 URLs sur 49 ont d√©j√† √©t√© scrap√©es et retir√©es du batch 2.\n",
      "‚è±Ô∏è Temps d'ex√©cution du scraping sur le batch2 : 665.7 secondes.\n",
      "‚è±Ô∏è Soit un total de 13.6 secondes par URL.\n",
      "‚úÖ Batch 2 enregistr√© dans scrap_batch_2.csv.\n",
      "On attend 30 minutes\n",
      "\n",
      "üéâ AU FINAL 98 URLs ont √©t√© scrap√©es avec succ√®s !\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 49\n",
    "num_batches = len(df_a_scraper) // batch_size + (1 if len(df_a_scraper) % batch_size != 0 else 0)\n",
    "print(f\"üì¶ TAILLE DES BATCHS = {batch_size}\")\n",
    "\n",
    "#1) Obtenir les URLs d√©j√† scrap√©es\n",
    "print(f\"üìÇ Nous avons d√©ja {len(urls_deja_scrapees)} URLs dans le dataset Fragrantica.\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "num_url_scrap = 0\n",
    "#2) It√©rer sur les batches \n",
    "for i in range(num_batches):\n",
    "    print(f\"\\nüì° Traitement du batch {i+1}/{num_batches}...\")\n",
    "\n",
    "    batch = df_a_scraper.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "    liste_url = batch[\"Lien du Parfum\"].tolist()\n",
    "\n",
    "    # üîç Supprimer les URLs d√©j√† scrap√©es\n",
    "    old_len = len(liste_url)\n",
    "    urls_deja_scrapees = Fragrantica_dataset['url'].tolist()\n",
    "    liste_url = [url for url in liste_url if url not in urls_deja_scrapees]\n",
    "    new_len = len(liste_url)\n",
    "    print(f\"üîç {old_len - new_len} URLs sur {old_len} ont d√©j√† √©t√© scrap√©es et retir√©es du batch {i+1}.\")\n",
    "\n",
    "    if not liste_url:  # Si aucun URL √† scraper, passer au batch suivant\n",
    "        print(f\"‚è≠Ô∏è Batch {i+1} ignor√© (toutes les URLs √©taient d√©j√† scrap√©es).\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        scraped_data = sip.scraping_multi_perfume_info(liste_url)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"‚è±Ô∏è Temps d'ex√©cution du scraping sur le batch{i+1} : {execution_time:.1f} secondes.\")\n",
    "        print(f\"‚è±Ô∏è Soit un total de {execution_time/len(liste_url):.1f} secondes par URL.\")\n",
    "        \n",
    "        # V√©rifier si le batch est vide\n",
    "\n",
    "        '''\n",
    "        cette condition n'est jamais remplie\n",
    "        '''\n",
    "        if scraped_data['nom_parfum'].empty:\n",
    "            print(f\"üö® Erreur critique : Le batch {i+1} est vide. On fait une grosse pause de 1H\")\n",
    "            time.sleep(3601)  # Pause de 5 minutes pour √©viter d'√™tre bloqu√© par le site\n",
    "            continue\n",
    "        # V√©rifier si le scrappinng c'es mal pass√©\n",
    "        if scraped_data['nom_parfum'].isnull().sum() > 0:\n",
    "            print(f\"üö® Erreur critique : Le batch {i+1} contient {scraped_data['nom_parfum'].isnull().sum()} lignes vides. On fait une grosse pause de 1H\")\n",
    "            # Filtrer les lignes valides\n",
    "            scraped_data = scraped_data.dropna(subset=['nom_parfum'])\n",
    "    \n",
    "            file_name = f\"scrap_batch_{i+1}.csv\"\n",
    "            scraped_data.to_csv(file_name, index=False) # Sauvegarde du batch sans index inutile\n",
    "            print(f\"‚úÖ Batch {i+1} enregistr√© dans {file_name} avec les lignes valides.\")\n",
    "            num_url_scrap += len(scraped_data)\n",
    "            time.sleep(3601)  # Pause de 5 minutes pour √©viter d'√™tre bloqu√© par le site\n",
    "            continue\n",
    "            '''        \n",
    "            print(f\"üö® Erreur critique : Le batch {i+1} contient des lignes vides. Arr√™t imm√©diat du script.\")\n",
    "            sys.exit(1)  # üíÄ Arr√™t imm√©diat du script avec un code d'erreur 1\n",
    "            '''\n",
    "\n",
    "        else:\n",
    "            file_name = f\"scrap_batch_{i+1}.csv\"\n",
    "            scraped_data.to_csv(file_name, index=False) # Sauvegarde du batch sans index inutile\n",
    "            print(f\"‚úÖ Batch {i+1} enregistr√© dans {file_name}.\")\n",
    "            num_url_scrap += new_len\n",
    "\n",
    "        print(\"On attend 30 minutes\")\n",
    "        time.sleep(1801)  # Pause pour √©viter d'√™tre bloqu√© par le site\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors du scraping du batch {i+1}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nüéâ AU FINAL {num_url_scrap} URLs ont √©t√© scrap√©es avec succ√®s !\")\n",
    "\n",
    "#3) Regarder le dernier batch\n",
    "scraped_data = pd.DataFrame()\n",
    "scraped_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code pour ajouter nos nouvelles lignes √† 'Fragrantica_dataset.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Les donn√©es du batch 1 ont √©t√© ajout√©es √† Fragrantica_dataset.csv sans doublons.\n",
      "‚úÖ Les donn√©es du batch 2 ont √©t√© ajout√©es √† Fragrantica_dataset.csv sans doublons.\n"
     ]
    }
   ],
   "source": [
    "num_premier_batch = 1\n",
    "num_dernier_batch = 2\n",
    "for i in range(num_premier_batch-1, num_dernier_batch):\n",
    "    scraped_data = pd.read_csv(f\"scrap_batch_{i+1}.csv\")\n",
    "    Fragrantica_dataset = pd.read_csv(\"Fragrantica_dataset.csv\")\n",
    "\n",
    "    # Concat√©ner les deux DataFrames\n",
    "    combined_data = pd.concat([Fragrantica_dataset, scraped_data])\n",
    "\n",
    "    # Supprimer les doublons\n",
    "    combined_data.drop_duplicates(subset=['url'], inplace=True)\n",
    "\n",
    "    # Sauvegarder le dataset combin√©\n",
    "    combined_data.to_csv(\"Fragrantica_dataset.csv\", index=False)\n",
    "    print(f\"‚úÖ Les donn√©es du batch {i+1} ont √©t√© ajout√©es √† Fragrantica_dataset.csv sans doublons.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
